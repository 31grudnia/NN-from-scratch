{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from nn_with_py.NNModels.Model import Model\n",
    "from nn_with_py.Layers.LayerDense import Layer_Dense\n",
    "from nn_with_py.Layers.LayerDropout import Layer_Dropout\n",
    "from nn_with_py.ActivationFunctions.ActivationRelu import Activation_Relu\n",
    "from nn_with_py.ActivationFunctions.ActivationSoftmax import Activation_Softmax\n",
    "from nn_with_py.LossFunctions.LossCategoricalCrossentropy import Loss_CategoricalCrossentropy\n",
    "from nn_with_py.Optimizers.OptimizerAdam import Optimizer_Adam\n",
    "from nn_with_py.Accuracy.AccuracyCategorical import Accuracy_Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/mikolajstarczewski/Desktop/Magisterka/NN_with_py/nn_with_py/data_files/\"\n",
    "\n",
    "X_train = np.load(PATH + \"X_train.npy\")\n",
    "X_test = np.load(PATH + \"X_test.npy\")\n",
    "Y_train = np.load(PATH + \"Y_train.npy\")\n",
    "Y_test = np.load(PATH + \"Y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446524, 1024)\n",
      "(1024,)\n",
      "1024\n",
      "(55815, 1024)\n",
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "# Flattening Data\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[0].shape)\n",
    "print(32*32)\n",
    "print(X_test.shape)\n",
    "print(X_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_w_py = Model()\n",
    "\n",
    "nn_model_w_py.add(Layer_Dense(X_train.shape[1], 1024, \n",
    "                              weight_regularizer_l2=5e-6, bias_regularizer_l2=5e-6))\n",
    "nn_model_w_py.add(Activation_Relu())\n",
    "nn_model_w_py.add(Layer_Dense(1024, 512))\n",
    "nn_model_w_py.add(Activation_Relu())\n",
    "nn_model_w_py.add(Layer_Dense(512, 256))\n",
    "nn_model_w_py.add(Activation_Relu())\n",
    "nn_model_w_py.add(Layer_Dropout(0.2))\n",
    "nn_model_w_py.add(Layer_Dense(256, 128))\n",
    "nn_model_w_py.add(Activation_Relu())\n",
    "nn_model_w_py.add(Layer_Dropout(0.2))\n",
    "nn_model_w_py.add(Layer_Dense(128, 89))\n",
    "nn_model_w_py.add(Activation_Softmax())\n",
    "\n",
    "nn_model_w_py.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    optimizer=Optimizer_Adam(decay=1e-6),\n",
    "    accuracy=Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "nn_model_w_py.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.008, loss: 4.489 (data_loss: 4.489, reg_loss: 0.001), lr: 0.001\n",
      "step: 1000, acc: 0.203, loss: 2.660 (data_loss: 2.659, reg_loss: 0.001), lr: 0.0009990009990009992\n",
      "step: 2000, acc: 0.453, loss: 1.954 (data_loss: 1.953, reg_loss: 0.002), lr: 0.000998003992015968\n",
      "step: 3000, acc: 0.492, loss: 1.417 (data_loss: 1.415, reg_loss: 0.002), lr: 0.0009970089730807579\n",
      "step: 3488, acc: 0.533, loss: 1.341 (data_loss: 1.338, reg_loss: 0.003), lr: 0.0009965241238559903\n",
      "training, acc: 0.365, loss: 2.235 (data_loss: 2.232, reg_loss: 0.003), lr: 0.0009965241238559903\n",
      "validation, acc: 0.606, loss: 1.229\n",
      "epoch: 2\n",
      "step: 0, acc: 0.617, loss: 1.198 (data_loss: 1.195, reg_loss: 0.003), lr: 0.0009965231307966504\n",
      "step: 1000, acc: 0.555, loss: 1.304 (data_loss: 1.301, reg_loss: 0.003), lr: 0.0009955310610668708\n",
      "step: 2000, acc: 0.633, loss: 1.158 (data_loss: 1.154, reg_loss: 0.004), lr: 0.0009945409646450632\n",
      "step: 3000, acc: 0.664, loss: 0.990 (data_loss: 0.986, reg_loss: 0.004), lr: 0.0009935528356494706\n",
      "step: 3488, acc: 0.583, loss: 1.091 (data_loss: 1.087, reg_loss: 0.004), lr: 0.0009930713412520842\n",
      "training, acc: 0.631, loss: 1.164 (data_loss: 1.159, reg_loss: 0.004), lr: 0.0009930713412520842\n",
      "validation, acc: 0.700, loss: 0.904\n",
      "epoch: 3\n",
      "step: 0, acc: 0.711, loss: 0.865 (data_loss: 0.860, reg_loss: 0.004), lr: 0.0009930703550623749\n",
      "step: 1000, acc: 0.602, loss: 1.045 (data_loss: 1.040, reg_loss: 0.005), lr: 0.00099208514471546\n",
      "step: 2000, acc: 0.664, loss: 1.105 (data_loss: 1.100, reg_loss: 0.005), lr: 0.0009911018872562137\n",
      "step: 3000, acc: 0.664, loss: 0.892 (data_loss: 0.886, reg_loss: 0.005), lr: 0.000990120576883853\n",
      "step: 3488, acc: 0.617, loss: 1.062 (data_loss: 1.056, reg_loss: 0.006), lr: 0.0009896424026142393\n",
      "training, acc: 0.685, loss: 0.970 (data_loss: 0.964, reg_loss: 0.006), lr: 0.0009896424026142393\n",
      "validation, acc: 0.722, loss: 0.831\n",
      "epoch: 4\n",
      "step: 0, acc: 0.688, loss: 0.961 (data_loss: 0.955, reg_loss: 0.006), lr: 0.0009896414232231235\n",
      "step: 1000, acc: 0.688, loss: 0.919 (data_loss: 0.913, reg_loss: 0.006), lr: 0.0009886630013633663\n",
      "step: 2000, acc: 0.695, loss: 1.009 (data_loss: 1.002, reg_loss: 0.006), lr: 0.0009876865122517573\n",
      "step: 3000, acc: 0.719, loss: 0.796 (data_loss: 0.789, reg_loss: 0.007), lr: 0.0009867119501670999\n",
      "step: 3488, acc: 0.633, loss: 0.995 (data_loss: 0.988, reg_loss: 0.007), lr: 0.0009862370618025456\n",
      "training, acc: 0.708, loss: 0.889 (data_loss: 0.882, reg_loss: 0.007), lr: 0.0009862370618025456\n",
      "validation, acc: 0.732, loss: 0.796\n",
      "epoch: 5\n",
      "step: 0, acc: 0.719, loss: 0.896 (data_loss: 0.889, reg_loss: 0.007), lr: 0.0009862360891399627\n",
      "step: 1000, acc: 0.672, loss: 0.827 (data_loss: 0.820, reg_loss: 0.007), lr: 0.0009852643858452978\n",
      "step: 2000, acc: 0.664, loss: 1.013 (data_loss: 1.006, reg_loss: 0.008), lr: 0.0009842945954352353\n",
      "step: 3000, acc: 0.750, loss: 0.722 (data_loss: 0.714, reg_loss: 0.008), lr: 0.0009833267122668042\n",
      "step: 3488, acc: 0.600, loss: 0.915 (data_loss: 0.907, reg_loss: 0.008), lr: 0.0009828550760533259\n",
      "training, acc: 0.722, loss: 0.842 (data_loss: 0.834, reg_loss: 0.008), lr: 0.0009828550760533259\n",
      "validation, acc: 0.742, loss: 0.753\n",
      "epoch: 6\n",
      "step: 0, acc: 0.734, loss: 0.881 (data_loss: 0.872, reg_loss: 0.008), lr: 0.0009828541100501747\n",
      "step: 1000, acc: 0.727, loss: 0.821 (data_loss: 0.813, reg_loss: 0.009), lr: 0.0009818890563555224\n",
      "step: 2000, acc: 0.727, loss: 0.886 (data_loss: 0.877, reg_loss: 0.009), lr: 0.0009809258959531902\n",
      "step: 3000, acc: 0.695, loss: 0.752 (data_loss: 0.743, reg_loss: 0.009), lr: 0.0009799646232770997\n",
      "step: 3488, acc: 0.567, loss: 0.869 (data_loss: 0.860, reg_loss: 0.009), lr: 0.0009794962059214462\n",
      "training, acc: 0.731, loss: 0.809 (data_loss: 0.799, reg_loss: 0.009), lr: 0.0009794962059214462\n",
      "validation, acc: 0.748, loss: 0.734\n",
      "epoch: 7\n",
      "step: 0, acc: 0.750, loss: 0.853 (data_loss: 0.844, reg_loss: 0.009), lr: 0.0009794952465095688\n",
      "step: 1000, acc: 0.703, loss: 0.795 (data_loss: 0.785, reg_loss: 0.010), lr: 0.0009785367743905184\n",
      "step: 2000, acc: 0.695, loss: 0.909 (data_loss: 0.899, reg_loss: 0.010), lr: 0.0009775801762381543\n",
      "step: 3000, acc: 0.734, loss: 0.636 (data_loss: 0.625, reg_loss: 0.011), lr: 0.0009766254465619856\n",
      "step: 3488, acc: 0.650, loss: 0.814 (data_loss: 0.803, reg_loss: 0.011), lr: 0.0009761602152238043\n",
      "training, acc: 0.738, loss: 0.788 (data_loss: 0.777, reg_loss: 0.011), lr: 0.0009761602152238043\n",
      "validation, acc: 0.754, loss: 0.707\n",
      "epoch: 8\n",
      "step: 0, acc: 0.789, loss: 0.850 (data_loss: 0.840, reg_loss: 0.011), lr: 0.0009761592623359686\n",
      "step: 1000, acc: 0.664, loss: 0.862 (data_loss: 0.851, reg_loss: 0.011), lr: 0.0009752073046927952\n",
      "step: 2000, acc: 0.711, loss: 0.981 (data_loss: 0.969, reg_loss: 0.011), lr: 0.0009742572019528011\n",
      "step: 3000, acc: 0.773, loss: 0.660 (data_loss: 0.648, reg_loss: 0.012), lr: 0.0009733089486998052\n",
      "step: 3488, acc: 0.683, loss: 1.048 (data_loss: 1.036, reg_loss: 0.012), lr: 0.0009728468709839666\n",
      "training, acc: 0.742, loss: 0.772 (data_loss: 0.761, reg_loss: 0.012), lr: 0.0009728468709839666\n",
      "validation, acc: 0.750, loss: 0.735\n",
      "epoch: 9\n",
      "step: 0, acc: 0.773, loss: 0.888 (data_loss: 0.876, reg_loss: 0.012), lr: 0.0009728459245538529\n",
      "step: 1000, acc: 0.711, loss: 0.701 (data_loss: 0.689, reg_loss: 0.012), lr: 0.0009719004151958573\n",
      "step: 2000, acc: 0.703, loss: 0.980 (data_loss: 0.967, reg_loss: 0.013), lr: 0.0009709567419352334\n",
      "step: 3000, acc: 0.719, loss: 0.756 (data_loss: 0.743, reg_loss: 0.013), lr: 0.0009700148994288552\n",
      "step: 3488, acc: 0.683, loss: 0.844 (data_loss: 0.831, reg_loss: 0.013), lr: 0.0009695559433779328\n",
      "training, acc: 0.747, loss: 0.757 (data_loss: 0.744, reg_loss: 0.013), lr: 0.0009695559433779328\n",
      "validation, acc: 0.761, loss: 0.691\n",
      "epoch: 10\n",
      "step: 0, acc: 0.781, loss: 0.775 (data_loss: 0.762, reg_loss: 0.013), lr: 0.000969555003340117\n",
      "step: 1000, acc: 0.688, loss: 0.797 (data_loss: 0.783, reg_loss: 0.013), lr: 0.0009686158769702858\n",
      "step: 2000, acc: 0.734, loss: 0.823 (data_loss: 0.810, reg_loss: 0.014), lr: 0.0009676785681453763\n",
      "step: 3000, acc: 0.750, loss: 0.672 (data_loss: 0.658, reg_loss: 0.014), lr: 0.0009667430715940918\n",
      "step: 3488, acc: 0.617, loss: 0.864 (data_loss: 0.850, reg_loss: 0.014), lr: 0.0009662872056809958\n",
      "training, acc: 0.750, loss: 0.747 (data_loss: 0.733, reg_loss: 0.014), lr: 0.0009662872056809958\n",
      "validation, acc: 0.763, loss: 0.687\n",
      "epoch: 11\n",
      "step: 0, acc: 0.750, loss: 0.764 (data_loss: 0.750, reg_loss: 0.014), lr: 0.000966286271970934\n",
      "step: 1000, acc: 0.750, loss: 0.714 (data_loss: 0.699, reg_loss: 0.014), lr: 0.0009653534641709062\n",
      "step: 2000, acc: 0.672, loss: 0.894 (data_loss: 0.880, reg_loss: 0.014), lr: 0.0009644224556124564\n",
      "step: 3000, acc: 0.758, loss: 0.696 (data_loss: 0.681, reg_loss: 0.015), lr: 0.0009634932410949137\n",
      "step: 3488, acc: 0.667, loss: 0.837 (data_loss: 0.822, reg_loss: 0.015), lr: 0.000963040434215671\n",
      "training, acc: 0.753, loss: 0.737 (data_loss: 0.722, reg_loss: 0.015), lr: 0.000963040434215671\n",
      "validation, acc: 0.770, loss: 0.669\n",
      "epoch: 12\n",
      "step: 0, acc: 0.797, loss: 0.758 (data_loss: 0.743, reg_loss: 0.015), lr: 0.0009630395067696863\n",
      "step: 1000, acc: 0.727, loss: 0.739 (data_loss: 0.724, reg_loss: 0.015), lr: 0.0009621129539850238\n",
      "step: 2000, acc: 0.695, loss: 0.892 (data_loss: 0.877, reg_loss: 0.015), lr: 0.0009611881823835353\n",
      "step: 3000, acc: 0.719, loss: 0.620 (data_loss: 0.604, reg_loss: 0.016), lr: 0.0009602651868339959\n",
      "step: 3488, acc: 0.667, loss: 0.674 (data_loss: 0.658, reg_loss: 0.016), lr: 0.0009598154083006756\n",
      "training, acc: 0.754, loss: 0.731 (data_loss: 0.715, reg_loss: 0.016), lr: 0.0009598154083006756\n",
      "validation, acc: 0.767, loss: 0.671\n",
      "epoch: 13\n",
      "step: 0, acc: 0.789, loss: 0.710 (data_loss: 0.694, reg_loss: 0.016), lr: 0.0009598144870559418\n",
      "step: 1000, acc: 0.766, loss: 0.752 (data_loss: 0.736, reg_loss: 0.016), lr: 0.0009588941265816959\n",
      "step: 2000, acc: 0.711, loss: 0.901 (data_loss: 0.885, reg_loss: 0.017), lr: 0.0009579755294730751\n",
      "step: 3000, acc: 0.727, loss: 0.738 (data_loss: 0.721, reg_loss: 0.017), lr: 0.0009570586906671465\n",
      "step: 3488, acc: 0.683, loss: 0.636 (data_loss: 0.619, reg_loss: 0.017), lr: 0.0009566119102009269\n",
      "training, acc: 0.757, loss: 0.723 (data_loss: 0.706, reg_loss: 0.017), lr: 0.0009566119102009269\n",
      "validation, acc: 0.766, loss: 0.681\n",
      "epoch: 14\n",
      "step: 0, acc: 0.758, loss: 0.737 (data_loss: 0.720, reg_loss: 0.017), lr: 0.0009566109950954554\n",
      "step: 1000, acc: 0.734, loss: 0.750 (data_loss: 0.732, reg_loss: 0.017), lr: 0.0009556967650620199\n",
      "step: 2000, acc: 0.734, loss: 0.864 (data_loss: 0.847, reg_loss: 0.017), lr: 0.0009547842808135143\n",
      "step: 3000, acc: 0.766, loss: 0.688 (data_loss: 0.670, reg_loss: 0.018), lr: 0.0009538735373541647\n",
      "step: 3488, acc: 0.650, loss: 0.780 (data_loss: 0.762, reg_loss: 0.018), lr: 0.0009534297250785388\n",
      "training, acc: 0.759, loss: 0.715 (data_loss: 0.698, reg_loss: 0.018), lr: 0.0009534297250785388\n",
      "validation, acc: 0.768, loss: 0.672\n",
      "epoch: 15\n",
      "step: 0, acc: 0.820, loss: 0.651 (data_loss: 0.633, reg_loss: 0.018), lr: 0.0009534288160511648\n",
      "step: 1000, acc: 0.734, loss: 0.775 (data_loss: 0.757, reg_loss: 0.018), lr: 0.0009525206554104126\n",
      "step: 2000, acc: 0.727, loss: 0.909 (data_loss: 0.890, reg_loss: 0.018), lr: 0.0009516142232068258\n",
      "step: 3000, acc: 0.789, loss: 0.597 (data_loss: 0.579, reg_loss: 0.019), lr: 0.0009507095145106793\n",
      "step: 3488, acc: 0.650, loss: 0.640 (data_loss: 0.621, reg_loss: 0.019), lr: 0.0009502686409447951\n",
      "training, acc: 0.761, loss: 0.712 (data_loss: 0.693, reg_loss: 0.019), lr: 0.0009502686409447951\n",
      "validation, acc: 0.762, loss: 0.696\n",
      "epoch: 16\n",
      "step: 0, acc: 0.758, loss: 0.783 (data_loss: 0.764, reg_loss: 0.019), lr: 0.0009502677379351631\n",
      "step: 1000, acc: 0.758, loss: 0.686 (data_loss: 0.667, reg_loss: 0.019), lr: 0.000949365586446857\n",
      "step: 2000, acc: 0.719, loss: 0.791 (data_loss: 0.772, reg_loss: 0.019), lr: 0.0009484651462770371\n",
      "step: 3000, acc: 0.789, loss: 0.536 (data_loss: 0.517, reg_loss: 0.019), lr: 0.0009475664125609404\n",
      "step: 3488, acc: 0.683, loss: 0.702 (data_loss: 0.683, reg_loss: 0.019), lr: 0.0009471284486130726\n",
      "training, acc: 0.763, loss: 0.706 (data_loss: 0.686, reg_loss: 0.019), lr: 0.0009471284486130726\n",
      "validation, acc: 0.769, loss: 0.672\n",
      "epoch: 17\n",
      "step: 0, acc: 0.781, loss: 0.738 (data_loss: 0.718, reg_loss: 0.019), lr: 0.0009471275515616239\n",
      "step: 1000, acc: 0.758, loss: 0.688 (data_loss: 0.668, reg_loss: 0.020), lr: 0.0009462313497800958\n",
      "step: 2000, acc: 0.719, loss: 0.916 (data_loss: 0.896, reg_loss: 0.020), lr: 0.0009453368424236923\n",
      "step: 3000, acc: 0.773, loss: 0.622 (data_loss: 0.602, reg_loss: 0.020), lr: 0.0009444440246915447\n",
      "step: 3488, acc: 0.783, loss: 0.675 (data_loss: 0.655, reg_loss: 0.020), lr: 0.0009440089416526954\n",
      "training, acc: 0.764, loss: 0.701 (data_loss: 0.681, reg_loss: 0.020), lr: 0.0009440089416526954\n",
      "validation, acc: 0.764, loss: 0.690\n",
      "epoch: 18\n",
      "step: 0, acc: 0.805, loss: 0.760 (data_loss: 0.740, reg_loss: 0.020), lr: 0.0009440080505006547\n",
      "step: 1000, acc: 0.695, loss: 0.791 (data_loss: 0.770, reg_loss: 0.020), lr: 0.0009431177397617495\n",
      "step: 2000, acc: 0.734, loss: 0.942 (data_loss: 0.921, reg_loss: 0.021), lr: 0.0009422291067762291\n",
      "step: 3000, acc: 0.766, loss: 0.584 (data_loss: 0.563, reg_loss: 0.021), lr: 0.0009413421468060731\n",
      "step: 3488, acc: 0.683, loss: 0.738 (data_loss: 0.717, reg_loss: 0.021), lr: 0.0009409099163436995\n",
      "training, acc: 0.766, loss: 0.697 (data_loss: 0.676, reg_loss: 0.021), lr: 0.0009409099163436995\n",
      "validation, acc: 0.774, loss: 0.651\n",
      "epoch: 19\n",
      "step: 0, acc: 0.836, loss: 0.650 (data_loss: 0.629, reg_loss: 0.021), lr: 0.0009409090310330617\n",
      "step: 1000, acc: 0.711, loss: 0.682 (data_loss: 0.661, reg_loss: 0.021), lr: 0.000940024553441336\n",
      "step: 2000, acc: 0.734, loss: 0.733 (data_loss: 0.712, reg_loss: 0.021), lr: 0.000939141737149254\n",
      "step: 3000, acc: 0.766, loss: 0.613 (data_loss: 0.591, reg_loss: 0.022), lr: 0.0009382605774806203\n",
      "step: 3488, acc: 0.700, loss: 0.754 (data_loss: 0.733, reg_loss: 0.022), lr: 0.0009378311716324827\n",
      "training, acc: 0.767, loss: 0.693 (data_loss: 0.672, reg_loss: 0.022), lr: 0.0009378311716324827\n",
      "validation, acc: 0.771, loss: 0.657\n",
      "epoch: 20\n",
      "step: 0, acc: 0.750, loss: 0.804 (data_loss: 0.782, reg_loss: 0.022), lr: 0.000937830292106001\n",
      "step: 1000, acc: 0.766, loss: 0.709 (data_loss: 0.687, reg_loss: 0.022), lr: 0.0009369515905221725\n",
      "step: 2000, acc: 0.688, loss: 0.927 (data_loss: 0.905, reg_loss: 0.022), lr: 0.0009360745339986953\n",
      "step: 3000, acc: 0.727, loss: 0.614 (data_loss: 0.592, reg_loss: 0.022), lr: 0.000935199117920192\n",
      "step: 3488, acc: 0.717, loss: 0.709 (data_loss: 0.686, reg_loss: 0.022), lr: 0.0009347725090883257\n",
      "training, acc: 0.769, loss: 0.688 (data_loss: 0.666, reg_loss: 0.022), lr: 0.0009347725090883257\n",
      "validation, acc: 0.775, loss: 0.658\n",
      "epoch: 21\n",
      "step: 0, acc: 0.820, loss: 0.704 (data_loss: 0.682, reg_loss: 0.022), lr: 0.0009347716352894988\n",
      "step: 1000, acc: 0.750, loss: 0.660 (data_loss: 0.637, reg_loss: 0.023), lr: 0.0009338986533181418\n",
      "step: 2000, acc: 0.797, loss: 0.828 (data_loss: 0.805, reg_loss: 0.023), lr: 0.0009330273003788091\n",
      "step: 3000, acc: 0.828, loss: 0.525 (data_loss: 0.502, reg_loss: 0.023), lr: 0.0009321575719159566\n",
      "step: 3488, acc: 0.717, loss: 0.706 (data_loss: 0.682, reg_loss: 0.023), lr: 0.0009317337328607579\n",
      "training, acc: 0.770, loss: 0.687 (data_loss: 0.664, reg_loss: 0.023), lr: 0.0009317337328607579\n",
      "validation, acc: 0.769, loss: 0.675\n",
      "epoch: 22\n",
      "step: 0, acc: 0.797, loss: 0.649 (data_loss: 0.625, reg_loss: 0.023), lr: 0.0009317328647338179\n",
      "step: 1000, acc: 0.734, loss: 0.719 (data_loss: 0.695, reg_loss: 0.023), lr: 0.0009308655467112986\n",
      "step: 2000, acc: 0.703, loss: 0.901 (data_loss: 0.878, reg_loss: 0.023), lr: 0.0009299998419000268\n",
      "step: 3000, acc: 0.781, loss: 0.655 (data_loss: 0.632, reg_loss: 0.024), lr: 0.0009291357458033262\n",
      "step: 3488, acc: 0.683, loss: 0.664 (data_loss: 0.640, reg_loss: 0.024), lr: 0.0009287146496377549\n",
      "training, acc: 0.770, loss: 0.683 (data_loss: 0.660, reg_loss: 0.024), lr: 0.0009287146496377549\n",
      "validation, acc: 0.775, loss: 0.658\n",
      "epoch: 23\n",
      "step: 0, acc: 0.836, loss: 0.733 (data_loss: 0.709, reg_loss: 0.024), lr: 0.0009287137871276555\n",
      "step: 1000, acc: 0.727, loss: 0.709 (data_loss: 0.685, reg_loss: 0.024), lr: 0.0009278520781102993\n",
      "step: 2000, acc: 0.742, loss: 0.785 (data_loss: 0.761, reg_loss: 0.024), lr: 0.0009269919666876166\n",
      "step: 3000, acc: 0.766, loss: 0.552 (data_loss: 0.528, reg_loss: 0.024), lr: 0.0009261334484208499\n",
      "step: 3488, acc: 0.667, loss: 0.713 (data_loss: 0.688, reg_loss: 0.024), lr: 0.0009257150686047438\n",
      "training, acc: 0.771, loss: 0.680 (data_loss: 0.656, reg_loss: 0.024), lr: 0.0009257150686047438\n",
      "validation, acc: 0.771, loss: 0.669\n",
      "epoch: 24\n",
      "step: 0, acc: 0.828, loss: 0.786 (data_loss: 0.762, reg_loss: 0.024), lr: 0.0009257142116571488\n",
      "step: 1000, acc: 0.727, loss: 0.746 (data_loss: 0.722, reg_loss: 0.024), lr: 0.000924858057409639\n",
      "step: 2000, acc: 0.711, loss: 0.900 (data_loss: 0.875, reg_loss: 0.025), lr: 0.0009240034853411468\n",
      "step: 3000, acc: 0.766, loss: 0.551 (data_loss: 0.526, reg_loss: 0.025), lr: 0.0009231504910699037\n",
      "step: 3488, acc: 0.717, loss: 0.716 (data_loss: 0.691, reg_loss: 0.025), lr: 0.0009227348014044025\n",
      "training, acc: 0.772, loss: 0.679 (data_loss: 0.654, reg_loss: 0.025), lr: 0.0009227348014044025\n",
      "validation, acc: 0.775, loss: 0.645\n",
      "epoch: 25\n",
      "step: 0, acc: 0.828, loss: 0.583 (data_loss: 0.558, reg_loss: 0.025), lr: 0.0009227339499656743\n",
      "step: 1000, acc: 0.734, loss: 0.784 (data_loss: 0.759, reg_loss: 0.025), lr: 0.0009218832969496726\n",
      "step: 2000, acc: 0.766, loss: 0.739 (data_loss: 0.714, reg_loss: 0.025), lr: 0.0009210342108947295\n",
      "step: 3000, acc: 0.758, loss: 0.528 (data_loss: 0.503, reg_loss: 0.025), lr: 0.000920186687475155\n",
      "step: 3488, acc: 0.700, loss: 0.640 (data_loss: 0.615, reg_loss: 0.025), lr: 0.0009197736620972311\n",
      "training, acc: 0.772, loss: 0.675 (data_loss: 0.650, reg_loss: 0.025), lr: 0.0009197736620972311\n",
      "validation, acc: 0.771, loss: 0.663\n",
      "epoch: 26\n",
      "step: 0, acc: 0.836, loss: 0.591 (data_loss: 0.565, reg_loss: 0.025), lr: 0.0009197728161144197\n",
      "step: 1000, acc: 0.742, loss: 0.772 (data_loss: 0.746, reg_loss: 0.026), lr: 0.0009189276114774059\n",
      "step: 2000, acc: 0.672, loss: 0.862 (data_loss: 0.836, reg_loss: 0.026), lr: 0.0009180839587780304\n",
      "step: 3000, acc: 0.781, loss: 0.587 (data_loss: 0.561, reg_loss: 0.026), lr: 0.0009172418537457864\n",
      "step: 3488, acc: 0.667, loss: 0.604 (data_loss: 0.578, reg_loss: 0.026), lr: 0.000916831467122882\n",
      "training, acc: 0.773, loss: 0.674 (data_loss: 0.648, reg_loss: 0.026), lr: 0.000916831467122882\n",
      "validation, acc: 0.772, loss: 0.664\n",
      "epoch: 27\n",
      "step: 0, acc: 0.812, loss: 0.737 (data_loss: 0.711, reg_loss: 0.026), lr: 0.0009168306265437136\n",
      "step: 1000, acc: 0.734, loss: 0.762 (data_loss: 0.736, reg_loss: 0.026), lr: 0.0009159908181080392\n",
      "step: 2000, acc: 0.711, loss: 0.813 (data_loss: 0.786, reg_loss: 0.026), lr: 0.0009151525467780225\n",
      "step: 3000, acc: 0.820, loss: 0.529 (data_loss: 0.502, reg_loss: 0.026), lr: 0.000914315808337463\n",
      "step: 3488, acc: 0.733, loss: 0.603 (data_loss: 0.576, reg_loss: 0.027), lr: 0.0009139080352622278\n",
      "training, acc: 0.774, loss: 0.671 (data_loss: 0.645, reg_loss: 0.027), lr: 0.0009139080352622278\n",
      "validation, acc: 0.767, loss: 0.686\n",
      "epoch: 28\n",
      "step: 0, acc: 0.781, loss: 0.648 (data_loss: 0.621, reg_loss: 0.027), lr: 0.000913907200035094\n",
      "step: 1000, acc: 0.734, loss: 0.634 (data_loss: 0.607, reg_loss: 0.027), lr: 0.0009130727362872454\n",
      "step: 2000, acc: 0.711, loss: 0.797 (data_loss: 0.770, reg_loss: 0.027), lr: 0.0009122397950014732\n",
      "step: 3000, acc: 0.789, loss: 0.552 (data_loss: 0.525, reg_loss: 0.027), lr: 0.0009114083720150237\n",
      "step: 3488, acc: 0.667, loss: 0.662 (data_loss: 0.635, reg_loss: 0.027), lr: 0.0009110031876001535\n",
      "training, acc: 0.775, loss: 0.669 (data_loss: 0.642, reg_loss: 0.027), lr: 0.0009110031876001535\n",
      "validation, acc: 0.774, loss: 0.663\n",
      "epoch: 29\n",
      "step: 0, acc: 0.789, loss: 0.679 (data_loss: 0.652, reg_loss: 0.027), lr: 0.0009110023576741018\n",
      "step: 1000, acc: 0.727, loss: 0.599 (data_loss: 0.571, reg_loss: 0.027), lr: 0.000910173187754166\n",
      "step: 2000, acc: 0.727, loss: 0.807 (data_loss: 0.779, reg_loss: 0.027), lr: 0.0009093455258381438\n",
      "step: 3000, acc: 0.789, loss: 0.650 (data_loss: 0.622, reg_loss: 0.028), lr: 0.0009085193678158831\n",
      "step: 3488, acc: 0.667, loss: 0.659 (data_loss: 0.631, reg_loss: 0.028), lr: 0.0009081167474890571\n",
      "training, acc: 0.776, loss: 0.667 (data_loss: 0.640, reg_loss: 0.028), lr: 0.0009081167474890571\n",
      "validation, acc: 0.778, loss: 0.653\n",
      "epoch: 30\n",
      "step: 0, acc: 0.812, loss: 0.726 (data_loss: 0.698, reg_loss: 0.028), lr: 0.0009081159228137791\n",
      "step: 1000, acc: 0.758, loss: 0.706 (data_loss: 0.679, reg_loss: 0.028), lr: 0.0009072919965051111\n",
      "step: 2000, acc: 0.742, loss: 0.889 (data_loss: 0.862, reg_loss: 0.028), lr: 0.0009064695639246869\n",
      "step: 3000, acc: 0.836, loss: 0.553 (data_loss: 0.525, reg_loss: 0.028), lr: 0.0009056486210141272\n",
      "step: 3488, acc: 0.683, loss: 0.727 (data_loss: 0.699, reg_loss: 0.028), lr: 0.0009052485405130406\n",
      "training, acc: 0.776, loss: 0.665 (data_loss: 0.637, reg_loss: 0.028), lr: 0.0009052485405130406\n",
      "validation, acc: 0.777, loss: 0.656\n"
     ]
    }
   ],
   "source": [
    "nn_model_w_py.train(X_train, Y_train, \n",
    "                    validation_data=(X_test, Y_test), \n",
    "                    epochs=30,  \n",
    "                    batch_size=128,\n",
    "                    print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <h3> Saving params (model needs to be initialized with same exact composition)\n",
    "- <h3> Saving whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PROPS_PATH = \"/Users/mikolajstarczewski/Desktop/Magisterka/NN_with_py/nn_with_py/NNModels/properties/\"\n",
    "\n",
    "nn_model_w_py.save_parameters(MODEL_PROPS_PATH + 'nn_model_w_py_params.parms')\n",
    "nn_model_w_py.save(MODEL_PROPS_PATH + 'nn_model_w_py_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.load(PATH + \"X_val.npy\")\n",
    "X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "Y_val = np.load(PATH + \"Y_val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for index in range(1, 2):\n",
    "    plt.title((Y_val[index]))\n",
    "    plt.imshow(X_val[index].reshape(32,32), cmap=cm.binary)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences = nn_model_w_py.predict(X_val[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.722, loss: 0.851\n"
     ]
    }
   ],
   "source": [
    "nn_model_w_py_v2 = Model()\n",
    "\n",
    "nn_model_w_py_v2.add(Layer_Dense(X_train.shape[1], 256))\n",
    "nn_model_w_py_v2.add(Activation_Relu())\n",
    "nn_model_w_py_v2.add(Layer_Dense(256, 128))\n",
    "nn_model_w_py_v2.add(Activation_Relu())\n",
    "nn_model_w_py_v2.add(Layer_Dense(128, 128))\n",
    "nn_model_w_py_v2.add(Activation_Relu())\n",
    "nn_model_w_py_v2.add(Layer_Dropout(0.2))\n",
    "nn_model_w_py_v2.add(Layer_Dense(128, 89))\n",
    "nn_model_w_py_v2.add(Activation_Softmax())\n",
    "\n",
    "nn_model_w_py_v2.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    accuracy=Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "nn_model_w_py_v2.finalize()\n",
    "nn_model_w_py_v2.load_parameters(MODEL_PROPS_PATH + 'nn_model_w_py_params.parms')\n",
    "nn_model_w_py_v2.evaluate(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.777, loss: 0.656\n"
     ]
    }
   ],
   "source": [
    "loaded_model = Model.load(MODEL_PROPS_PATH + 'nn_model_w_py_model.model')\n",
    "loaded_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value: 24\n",
      "Predicted: 24\n",
      "---------------\n",
      "True value: 77\n",
      "Predicted: 5\n",
      "---------------\n",
      "True value: 1\n",
      "Predicted: 1\n",
      "---------------\n",
      "True value: 83\n",
      "Predicted: 83\n",
      "---------------\n",
      "True value: 76\n",
      "Predicted: 76\n",
      "---------------\n",
      "True value: 6\n",
      "Predicted: 6\n",
      "---------------\n",
      "True value: 61\n",
      "Predicted: 61\n",
      "---------------\n",
      "True value: 27\n",
      "Predicted: 36\n",
      "---------------\n",
      "True value: 78\n",
      "Predicted: 74\n",
      "---------------\n",
      "True value: 86\n",
      "Predicted: 86\n",
      "---------------\n",
      "True value: 86\n",
      "Predicted: 86\n",
      "---------------\n",
      "True value: 39\n",
      "Predicted: 39\n",
      "---------------\n",
      "True value: 44\n",
      "Predicted: 44\n",
      "---------------\n",
      "True value: 77\n",
      "Predicted: 69\n",
      "---------------\n",
      "True value: 5\n",
      "Predicted: 5\n",
      "---------------\n",
      "True value: 71\n",
      "Predicted: 71\n",
      "---------------\n",
      "True value: 54\n",
      "Predicted: 54\n",
      "---------------\n",
      "True value: 81\n",
      "Predicted: 81\n",
      "---------------\n",
      "True value: 44\n",
      "Predicted: 44\n",
      "---------------\n",
      "True value: 88\n",
      "Predicted: 81\n",
      "---------------\n",
      "True value: 87\n",
      "Predicted: 87\n",
      "---------------\n",
      "True value: 63\n",
      "Predicted: 72\n",
      "---------------\n",
      "True value: 47\n",
      "Predicted: 47\n",
      "---------------\n",
      "True value: 18\n",
      "Predicted: 18\n",
      "---------------\n",
      "True value: 2\n",
      "Predicted: 2\n",
      "---------------\n",
      "True value: 45\n",
      "Predicted: 28\n",
      "---------------\n",
      "True value: 15\n",
      "Predicted: 15\n",
      "---------------\n",
      "True value: 74\n",
      "Predicted: 29\n",
      "---------------\n",
      "True value: 22\n",
      "Predicted: 22\n",
      "---------------\n",
      "True value: 39\n",
      "Predicted: 39\n",
      "---------------\n",
      "True value: 64\n",
      "Predicted: 14\n",
      "---------------\n",
      "True value: 56\n",
      "Predicted: 56\n",
      "---------------\n",
      "True value: 39\n",
      "Predicted: 39\n",
      "---------------\n",
      "True value: 85\n",
      "Predicted: 85\n",
      "---------------\n",
      "True value: 48\n",
      "Predicted: 48\n",
      "---------------\n",
      "True value: 66\n",
      "Predicted: 66\n",
      "---------------\n",
      "True value: 88\n",
      "Predicted: 88\n",
      "---------------\n",
      "True value: 39\n",
      "Predicted: 39\n",
      "---------------\n",
      "True value: 87\n",
      "Predicted: 87\n",
      "---------------\n",
      "True value: 70\n",
      "Predicted: 79\n",
      "---------------\n",
      "True value: 75\n",
      "Predicted: 75\n",
      "---------------\n",
      "True value: 88\n",
      "Predicted: 88\n",
      "---------------\n",
      "True value: 29\n",
      "Predicted: 80\n",
      "---------------\n",
      "True value: 86\n",
      "Predicted: 86\n",
      "---------------\n",
      "True value: 7\n",
      "Predicted: 7\n",
      "---------------\n",
      "True value: 79\n",
      "Predicted: 79\n",
      "---------------\n",
      "True value: 57\n",
      "Predicted: 19\n",
      "---------------\n",
      "True value: 37\n",
      "Predicted: 37\n",
      "---------------\n",
      "True value: 84\n",
      "Predicted: 26\n",
      "---------------\n",
      "True value: 56\n",
      "Predicted: 56\n",
      "---------------\n",
      "True value: 59\n",
      "Predicted: 59\n",
      "---------------\n",
      "True value: 55\n",
      "Predicted: 55\n",
      "---------------\n",
      "True value: 33\n",
      "Predicted: 33\n",
      "---------------\n",
      "True value: 77\n",
      "Predicted: 77\n",
      "---------------\n",
      "True value: 76\n",
      "Predicted: 76\n",
      "---------------\n",
      "True value: 16\n",
      "Predicted: 16\n",
      "---------------\n",
      "True value: 12\n",
      "Predicted: 12\n",
      "---------------\n",
      "True value: 22\n",
      "Predicted: 22\n",
      "---------------\n",
      "True value: 55\n",
      "Predicted: 55\n",
      "---------------\n",
      "True value: 26\n",
      "Predicted: 26\n",
      "---------------\n",
      "True value: 51\n",
      "Predicted: 51\n",
      "---------------\n",
      "True value: 11\n",
      "Predicted: 11\n",
      "---------------\n",
      "True value: 23\n",
      "Predicted: 23\n",
      "---------------\n",
      "True value: 51\n",
      "Predicted: 25\n",
      "---------------\n",
      "True value: 55\n",
      "Predicted: 55\n",
      "---------------\n",
      "True value: 8\n",
      "Predicted: 8\n",
      "---------------\n",
      "True value: 24\n",
      "Predicted: 24\n",
      "---------------\n",
      "True value: 19\n",
      "Predicted: 19\n",
      "---------------\n",
      "True value: 12\n",
      "Predicted: 38\n",
      "---------------\n",
      "True value: 13\n",
      "Predicted: 13\n",
      "---------------\n",
      "True value: 59\n",
      "Predicted: 59\n",
      "---------------\n",
      "True value: 29\n",
      "Predicted: 29\n",
      "---------------\n",
      "True value: 21\n",
      "Predicted: 21\n",
      "---------------\n",
      "True value: 52\n",
      "Predicted: 52\n",
      "---------------\n",
      "True value: 83\n",
      "Predicted: 83\n",
      "---------------\n",
      "True value: 40\n",
      "Predicted: 40\n",
      "---------------\n",
      "True value: 22\n",
      "Predicted: 22\n",
      "---------------\n",
      "True value: 20\n",
      "Predicted: 20\n",
      "---------------\n",
      "True value: 82\n",
      "Predicted: 82\n",
      "---------------\n",
      "True value: 85\n",
      "Predicted: 85\n",
      "---------------\n",
      "True value: 71\n",
      "Predicted: 71\n",
      "---------------\n",
      "True value: 39\n",
      "Predicted: 39\n",
      "---------------\n",
      "True value: 45\n",
      "Predicted: 45\n",
      "---------------\n",
      "True value: 22\n",
      "Predicted: 22\n",
      "---------------\n",
      "True value: 5\n",
      "Predicted: 5\n",
      "---------------\n",
      "True value: 9\n",
      "Predicted: 9\n",
      "---------------\n",
      "True value: 9\n",
      "Predicted: 16\n",
      "---------------\n",
      "True value: 84\n",
      "Predicted: 84\n",
      "---------------\n",
      "True value: 33\n",
      "Predicted: 33\n",
      "---------------\n",
      "True value: 31\n",
      "Predicted: 66\n",
      "---------------\n",
      "True value: 87\n",
      "Predicted: 87\n",
      "---------------\n",
      "True value: 35\n",
      "Predicted: 61\n",
      "---------------\n",
      "True value: 8\n",
      "Predicted: 8\n",
      "---------------\n",
      "True value: 50\n",
      "Predicted: 50\n",
      "---------------\n",
      "True value: 47\n",
      "Predicted: 47\n",
      "---------------\n",
      "True value: 85\n",
      "Predicted: 85\n",
      "---------------\n",
      "True value: 7\n",
      "Predicted: 86\n",
      "---------------\n",
      "True value: 8\n",
      "Predicted: 8\n",
      "---------------\n",
      "True value: 24\n",
      "Predicted: 50\n",
      "---------------\n",
      "True value: 74\n",
      "Predicted: 74\n",
      "---------------\n",
      "Mean 0.8\n"
     ]
    }
   ],
   "source": [
    "good_vals = 0\n",
    "all_vals = 100\n",
    "for i in range(0, all_vals):\n",
    "    confidences = loaded_model.predict(X_val[i])\n",
    "    print(f\"True value: {Y_val[i]}\")\n",
    "    print(f\"Predicted: {np.argmax(confidences)}\")\n",
    "    print(\"---------------\")\n",
    "    if np.argmax(confidences) == Y_val[i]:\n",
    "        good_vals += 1\n",
    "print(f\"Mean {(good_vals/all_vals):.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn-with-py-04WZxK0C-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
